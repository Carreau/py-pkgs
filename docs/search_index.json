[
["index.html", "Python packages Preface", " Python packages Tiffany Timbers &amp; Tomas Beuzen 2020-02-25 Preface This book is aimed at intermediate Python users who want to package up their code to share it with their collaborators (including their future selves) and the wider Python community. It’s scope and intent is inspired by the R packages book written by Hadley Wickham and Jenny Bryan. "],
["setup.html", "1 System setup 1.1 Installing and updating Python 1.2 Register for a PyPI account 1.3 Python IDEs 1.4 Setting up the RStudio IDE with Python", " 1 System setup 1.1 Installing and updating Python In order to start off on a good footing, we recommend you follow these system set-up instructions so you will run into fewer technical issues (compared to not using the same system this book was designed for). We recommend installing Python 3.7 via the Anaconda distribution, following the instructions here: https://docs.anaconda.com/anaconda/install/. If you already have installed this distribution, ensure Python, and the conda package manager is up to date by running the following commands: $ conda update --all Next, use conda to install Python poetry - a python package that will help us more efficiently build our Python packages: $ conda install -c conda-forge poetry Finally, we install cookiecutter - a python package that will create Python projects from project templates $ conda install -c conda-forge cookiecutter 1.2 Register for a PyPI account To publish a Python package on PyPI to share with others, you will need to register for a PyPI account. You can do so freely by clicking here: https://pypi.org/account/register/ Before we are ready to publish our packages on PyPI, it is advisable that we test drive them on TestPyPI first. To do so, you need to also register for a TestPyPI account. You can do so freely by clicking here: https://test.pypi.org/account/register/ 1.3 Python IDEs Commonly used free Python IDEs include Visual Studio Code, Atom, and PyCharm Community Edition. Visual Studio Code and Atom are text editors that can be customised with extensions to act as Python (or any other language) IDEs. In contrast, PyCharm is specifically a Python IDE and will work right out of the box - we describe setting up PyCharm briefly below. 1.3.1 PyCharm PyCharm offers a paid Professional version and free Community version. You can download either from https://www.jetbrains.com/pycharm/download/. Once downloaded, PyCharm will guide you through initial setup. We recommended using all default settings throughout the setup, with the exception of installing the Markdown Plugin when prompted to install “Featured Plugins”. Once setup is complete you should see something like the following screenshot: We now need to link Anaconda with PyCharm. Click Configure at the bottom right of the screen and then Preferences. Select Project Interpreter from the tab-menu and then click the gear icon to the right of the drop-down menu that appears and select Add…. In the pop-up menu that appears, click System Interpreter from the tab-menu and click the three dots … to the right of the drop-down menu. You now need to provide the path to Anaconda’s installation of Python, something like “/Users/user/anaconda3/bin/python”. You can determine the path using the following: Mac OS &amp; Linux: In terminal, type: which Python Windows: In the Anaconda Prompt type: where python. Copy and paste the path into the PyCharm pop-up. At this point, your window will probably look something like the one below. Click OK. Click OK again to get back to the Preferences menu, at which point PyCharm will show Anaconda’s installation of Python in the Project Interpreter drop-down menu and will populate the screen with the packages available to that interpreter (these will be all the packages installed in your base Anaconda environment) Click OK to return to the main menu. To start a new project you will click Create New Project. In the subsequent screen, an example of which is shown below, you may choose a location for your new project and you can also select an interpreter. Choose the Existing Interpreter radio button and then from the drop-down menu select Anaconda’s Python interpreter that we just set-up (this will likely be the only option in the drop-down menu). Click Create to get started. Note that PyCharm has excellent integration with Conda environments. If you wish to use a custom Conda environment for a project, you can easily create or select an existing environment to use as a project’s interpreter. To do this, in Step 3 above, simply click Conda Environment rather than System Interpreter and create or select an existing Conda environment. This environment will then be available to select as a project interpreter for new or existing projects. This was a brief, practical guide to getting started with PyCharm Community Edition. We recommend checking out the documentation for more guidance on setting up and using PyCharm. 1.4 Setting up the RStudio IDE with Python This book will use the RStudio integrated development environment (IDE) to develop Python packages - we use RStudio because in the UBC Master of Data Science program we teach both the R and Python programming languages and prefer to use an IDE that works well with both. However, any other Python IDE should work and we briefly describe some in [Alternative Python IDEs]. If you would like to use the RStudio IDE we recommend installing the most recent version of the IDE from the preview site: https://rstudio.com/products/rstudio/download/preview/ and then installing R from CRAN, and the reticulate R package via install.packages(\"reticulate\") from the R console inside RStudio. When installing reticulate, you will be prompted to install miniconda, if you have already installed the Anaconda distribution of Python, answer “no” to installing miniconda at this prompt. 1.4.1 Find where Anaconda is installed on your machine 1.4.1.1 Mac OS &amp; Linux In terminal, type: which Python 1.4.1.2 Windows In the Anaconda Prompt type: where python. 1.4.2 Configuring reticulate for to use the Python REPL inside RStudio Create a file named .Rprofile in your $HOME directory that contains the following: Sys.setenv(RETICULATE_PYTHON = &quot;path_to_the_folder_containing_anaconda&#39;s_python&quot;) For me the \"path_to_the_folder_containing_anaconda's_python\" was 'Users/user1/anaconda3/bin/Python' on a Mac OS. Restart RStudio for this to take effect. 1.4.3 Configuring the RStudio terminal 1.4.3.1 Mac OS &amp; Linux Open (or create) the file called .bash_profile in your $HOME directory and add the following to the last line of that file: export PATH=&quot;path_to_the_folder_containing_anaconda&#39;s_python:$PATH&quot; For me that line is export PATH=\"//anaconda3/bin:$PATH\". Restart RStudio for this to take effect. 1.4.3.2 Windows The default terminal in RStudio on Windows is PowerShell. This causes some unexpected problems as its not a true bash shell. You should change this using the following menu selections inside RStudio: Global Options -&gt; Terminal -&gt; Shell -&gt; Git Bash "],
["whole-game.html", "2 The Whole Game 2.1 Use Cookiecutter &amp; Poetry to create a Python project 2.2 Put your project under version control 2.3 Writing our first function 2.4 Test drive your package code 2.5 Add package function dependencies 2.6 Package documentation 2.7 Testing 2.8 Building your package and publishing to testPyPI", " 2 The Whole Game This chapter demonstrates how to develop an entire small toy Python package from beginning to end. It’s purpose is to give a high level overview of how a Python package can and should be developed. Later chapters will explore packaging in Python in more detail. This chapter is a Pythonified version of the Whole Game chapter written by Jenny Bryan that can be found in the the R packages book. 2.1 Use Cookiecutter &amp; Poetry to create a Python project So that we do not have to create a file and directory structure for our project from scratch, we will use Cookiecutter &amp; Poetry to do this for us (which you installed back in 1.1 Installing and updating Python. First, we use Cookiecutter to create the file and directory structure for our Python project (which will be a Python package). We will use a simplified version of the template base by the PyOpenSci organization designed specifically for creating Python packages. PyOpenSci is a not-for-profit organization that promotes open and reproducible research through peer-review of scientific Python packages. To use Cookiecutter to set up the structure of your Python package, run the line of code below in the terminal from the directory where you would like your package to live. $ cookiecutter https://github.com/UBC-MDS/cookiecutter-ubc-mds.git You will be prompted to provide information that will help customize the project. In this tutorial we will be calling our package foocat. However, we will eventually be publishing our package to testPyPI which is a testing version of Python’s main package index PyPI. Package names on testPyPI and PyPI must be unique. As a result, you will need to choose a unique name for your package while following this tutorial. Something like foocat_[your intials] might be appropriate, but you can always check if your chosen name is already taken by visitng testPyPI and searching that name. Below is an example of how to respond to the Cookiecutter prompts (default values for each attribute are shown in square brackets, hitting enter will accept the default attribute value): full_name [Audrey Roy Greenfeld]: Tiffany Timbers email [audreyr@example.com]: tiffany.timbers@gmail.com github_username [audreyr]: ttimbers project_name [Python Boilerplate]: foocat project_slug [foocat]: foocat project_short_description [Python Boilerplate contains all the boilerplate you need to create a Python package.]: Python package that eases the pain concatenating Pandas categoricals! pypi_username [ttimbers]: version [&#39;0.1.0&#39;]: Select open_source_license: 1 - MIT license 2 - BSD license 3 - ISC license 4 - Apache Software License 2.0 5 - GNU General Public License v3 Choose from 1, 2, 3, 4, 5 [1]: 1 You will now have a new directory called foocat. Navigate into the foocat directory and initialize the project as a Poetry project so that we can take advantage of the package management and building tools of Poetry: $ cd foocat $ poetry init Again we are prompted for more information. Once again, default values are shown in square brackets and have been populated where possible from our Cookiecutter template. Here is an example of how to respond the the prompts: This command will guide you through creating your pyproject.toml config. Package name [foocat]: Version [0.1.0]: Description []: Python package that eases the pain concatenating Pandas categoricals! Author [ttimbers &lt;tiffany.timbers@stat.ubc.ca&gt;, n to skip]: License []: MIT Compatible Python versions [^3.7]: Would you like to define your main dependencies interactively? (yes/no) [yes] no Would you like to define your dev dependencies (require-dev) interactively (yes/no) [yes] no Generated file [tool.poetry] name = &quot;foocat&quot; version = &quot;0.1.0&quot; description = &quot;Python package that eases the pain concatenating Pandas categoricals!&quot; authors = [&quot;ttimbers &lt;tiffany.timbers@stat.ubc.ca&gt;&quot;] license = &quot;MIT&quot; [tool.poetry.dependencies] python = &quot;^3.7&quot; [tool.poetry.dev-dependencies] [build-system] requires = [&quot;poetry&gt;=0.12&quot;] build-backend = &quot;poetry.masonry.api&quot; Do you confirm generation? (yes/no) [yes] Note - we said no to defining our dependencies interactively because it is more efficient to define them using poetry add which we will explore a bit later on. After using Cookiecutter and Poetry, we end up with the following directory structure: foocat ├── CONDUCT.md ├── CONTRIBUTING.md ├── CONTRIBUTORS.md ├── docs │ └── conf.py │ └── contributing.rst │ └── contributors.rst │ └── index.rst │ └── installation.rst │ └── Makefile │ └── readme.rst │ └── usage.rst ├── foocat │ └── __init__.py │ └── foocat.py ├── .github │ └── workflows │ └── build.yml │ └── release.yml ├── .gitignore ├── LICENSE ├── pyproject.toml └── tests ├── __init__.py └── test_foocat.py These two simple steps (Cookiecutter + Poetry) have given us a boilerplate file and directory structure suitable for building our Python package. While there are quite a few files in our boilerplate, at this point we only need to worry about a few of these files and directories to get a working package together. Specifically, we’ll be working on: the file for us to write out Python functions that our package will distribute (foocat/foocat.py); the home for our tests to ensure that our package functions work as we expect they should (tests/test_foocat.py); and, the pyproject.toml file that defines our project’s metadata and dependencies and how it will eventually be built and distributed. Later chapters will focus on the oter components of the boilerplate, which can be used to refine your package and packaging process with, for example, quality documentation, continuous integration testing, version bumping, etc. Optional for RStudio IDE users Users of the RStudio IDE may also want to make this Python project directory an RStudio project. Why might you ask? Well, once you have an *.Rproj file, you can use that file to quickly open the RStudio IDE (which has a terminal and an interactive Python REPL, assuming you have set this up with reticulate) to the project’s root directory. 2.2 Put your project under version control It is good practice to put your data science projects under local and remote version control. The tools we recommend using for this are Git &amp; GitHub. For this book, we assume readers have Git installed on their machine, have novice Git skills, and that have a GitHub.com account. 2.2.1 Set-up local version control From the terminal and in the root foocat directory, we will initialize the repository to be tracked by Git using: $ git init Initialized empty Git repository in /Users/tiffany/Documents/ubc-mds/foocat/.git/ Next, we need to tell Git which files to track (which will be all of them at this point) and commit these changes locally: $ git add . $ git commit -m &quot;initial project set-up&quot; [master (root-commit) ca03932] initial project set-up 22 files changed, 798 insertions(+) create mode 100644 .github/workflows/build.yml create mode 100644 .github/workflows/release.yml create mode 100644 .gitignore create mode 100644 CONDUCT.md create mode 100755 CONTRIBUTING.md create mode 100755 CONTRIBUTORS.md create mode 100755 LICENSE create mode 100644 README.md create mode 100755 docs/Makefile create mode 100755 docs/conf.py create mode 100755 docs/contributing.rst create mode 100755 docs/contributors.rst create mode 100755 docs/index.rst create mode 100755 docs/installation.rst create mode 100755 docs/make.bat create mode 100755 docs/readme.rst create mode 100755 docs/usage.rst create mode 100644 foocat/__init__.py create mode 100644 foocat/foocat.py create mode 100644 pyproject.toml create mode 100644 tests/__init__.py create mode 100644 tests/test_foocat.py 2.2.2 Set-up remote version control Now that we have set up our local version control, let’s create a repository on GitHub.com and set that as the remote version control home for this project: The options we recommend for setting up a repository for a Python package using the workflow we present in this book include: give the GitHub.com repository the same name as your Python Poetry project’s name make the GitHub.com repository public do not initialize the GitHub.com repository with a README Next, we set-up the remote address locally, and push our project to GitHub.com: $ git remote add origin git@github.com:ttimbers/foocat.git $ git push -u origin master Note: the example above uses SSH authentication with GitHub, HTTPS authentication works as well and would use this url in place of the one shown above to set the remote: https://github.com/ttimbers/foocat.git. 2.3 Writing our first function Pandas categoricals are a very useful data type for modeling (and were inspired by R’s factors), but certain manipulations of this data type can be tricky during data wrangling. One such challenge is concatenation (joining) of Pandas categoricals. Let’s observe the result of trying to concatenate two Pandas categoricals objects: &gt;&gt;&gt; import pandas as pd &gt;&gt;&gt; a = pd.Categorical([&quot;character&quot;, &quot;hits&quot;, &quot;your&quot;, &quot;eyeballs&quot;]) &gt;&gt;&gt; b = pd.Categorical([&quot;but&quot;, &quot;integer&quot;, &quot;where it&quot;, &quot;counts&quot;]) &gt;&gt;&gt; pd.concat([a, b]) ## Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: cannot concatenate object of type &#39;&lt;class &#39;pandas.core.arrays.categorical.Categorical&#39;&gt;&#39;; only Series and DataFrame objs are valid This occurs because the categoricals are represented as integers in memory, and in a the integer 0 corresponds to the word “character” while in b, the integer 0 corresponds to the word “but”. Thus, when we ask Python to concatenate these two Pandas categorical options it doesn’t know what to do with these integer mappings to different categories, and so it throws an error. We can get around this several ways, one way is to convert the Pandas categoricals to a str type, then do the concatenation, and finally convert the concatenated Pandas obeject to a categorical again. We demonstrate that approach below: &gt;&gt;&gt; concatenated = pd.concat([pd.Series(a.astype(&quot;str&quot;)), pd.Series(b.astype(&quot;str&quot;))]) &gt;&gt;&gt; pd.Categorical(concatenated) ## [character, hits, your, eyeballs, but, integer, where it, counts] ## Categories (8, object): [but, character, counts, eyeballs, hits, integer, where it, your] That seems to work 🎉, but it’s quite a bit of typing every time we want to do this… So let’s turn this code into a function called catbind! def catbind(a, b): concatenated = pd.concat([pd.Series(a.astype(&quot;str&quot;)), pd.Series(b.astype(&quot;str&quot;))]) return pd.Categorical(concatenated) catbind(a, b) ## [character, hits, your, eyeballs, but, integer, where it, counts] ## Categories (8, object): [but, character, counts, eyeballs, hits, integer, where it, your] Note - this book assumes you know how to write, document and test functions in Python. To learn more about this see Think Python, Chapter 3: Functions by Allen Downey. Where do we save this function if we want it to be a part of our foocat Python package? Let’s review the landscape of our Python project so far: foocat ├── CONDUCT.md ├── CONTRIBUTING.md ├── CONTRIBUTORS.md ├── docs │ └── conf.py │ └── contributing.rst │ └── contributors.rst │ └── index.rst │ └── installation.rst │ └── Makefile │ └── readme.rst │ └── usage.rst ├── foocat │ └── __init__.py │ └── foocat.py ├── .github │ └── workflows │ └── build.yml │ └── release.yml ├── .gitignore ├── LICENSE ├── pyproject.toml └── tests ├── __init__.py └── test_foocat.py All the code that we would like the user to run as part of our package should live inside the foocat directory. Typically, with a relatively small package with just a few functions, we would house them inside a single python module (i.e., a .py file). Our template project directory structure already created and named such a module for us: foocat/foocat.py. Let’s save our function there. Additionally, given that our package depends on the Pandas Python package, we should import Pandas at the top of the foocat.py file. Here’s what foocat.py should now look like: import pandas as pd def catbind(a, b): concatenated = pd.concat([pd.Series(a.astype(&quot;str&quot;)), pd.Series(b.astype(&quot;str&quot;))]) return pd.Categorical(concatenated) 2.4 Test drive your package code To test drive the function we just wrote we first install the package locally using Python poetry. We choose to do this with Python Poetry as opposed to using Python’s native package manager pip because Poetry automatically creates a virtual environment for us and will perform tricky tasks like package solving that can sometimes trip us up when we use pip alone. $ poetry install Creating virtualenv foocat-z0_J6H1I-py3.7 in /Users/tiffany/Library/Caches/pypoetry/virtualenvs Installing dependencies from lock file No dependencies to install or update - Installing foocat (0.1.0) Now, inside this project directory, we can open an interactive Python session (using python at the CL) and import our foocat module which contains our catbind function as shown: &gt;&gt;&gt; from foocat import foocat The foocat module has now been mapped to the current session’s namespace and we can access the catbind function in our Python session using dot notation: foocat.catbind (note that if you wanted to import a specific function, rather than the whole module, you could do from foocat.foocat import catbind, in which case “dot notation” would not be required to use the function). Let’s try to use the function to concatenate two Pandas categoricals: &gt;&gt;&gt; import pandas as pd &gt;&gt;&gt; a = pd.Categorical([&quot;character&quot;, &quot;hits&quot;, &quot;your&quot;, &quot;eyeballs&quot;]) &gt;&gt;&gt; b = pd.Categorical([&quot;but&quot;, &quot;integer&quot;, &quot;where it&quot;, &quot;counts&quot;]) &gt;&gt;&gt; foocat.catbind(a, b) [character, hits, your, eyeballs, but, integer, where it, counts] Categories (8, object): [but, character, counts, eyeballs, hits, integer, where it, your] Hurray again! This seems to work as expected! Now that we have something working, let’s commit this to version control: $ git add . $ git commit -m &quot;First working version of catbind function&quot; [master b80dea8] First working version of catbind function 1 file changed, 7 insertions(+) 2.5 Add package function dependencies Our function depends on the Pandas package, and without it, it would fail to work. Thus we need to record this dependency in a useful place so that when we publish our pacakged code this important information (and the mechanism for making it work) will be shipped along with it. We again use poetry to do this, using the add command. This command will update the [tool.poetry.dependencies] section of the pyproject.toml file which currently looks like this and lists only Python as a project dependency: [tool.poetry] name = &quot;foocat&quot; version = &quot;0.1.0&quot; description = &quot;Python package that eases the pain concatenating Pandas categoricals!&quot; authors = [&quot;ttimbers &lt;tiffany.timbers@stat.ubc.ca&gt;&quot;] license = &quot;MIT&quot; [tool.poetry.dependencies] python = &quot;^3.7&quot; [tool.poetry.dev-dependencies] [build-system] requires = [&quot;poetry&gt;=0.12&quot;] build-backend = &quot;poetry.masonry.api&quot; Let’s add our Pandas dependency now: $ poetry add pandas Using version ^1.0.1 for pandas Updating dependencies Resolving dependencies... (0.1s) Writing lock file Package operations: 4 installs, 1 update, 0 removals - Updating six (1.14.0 /usr/local/Cellar/poetry/1.0.3/libexec/vendor/lib/python3.7/site-packages -&gt; 1.14.0) - Installing numpy (1.18.1) - Installing python-dateutil (2.8.1) - Installing pytz (2019.3) - Installing pandas (1.0.1) Now if we view our pyproject.toml file we see that pandas is now listed as a dependency: [tool.poetry] name = &quot;foocat&quot; version = &quot;0.1.0&quot; description = &quot;Python package that eases the pain concatenating Pandas categoricals!&quot; authors = [&quot;ttimbers &lt;tiffany.timbers@stat.ubc.ca&gt;&quot;] license = &quot;MIT&quot; [tool.poetry.dependencies] python = &quot;^3.7&quot; pandas = &quot;^1.0.1&quot; [tool.poetry.dev-dependencies] [build-system] requires = [&quot;poetry&gt;=0.12&quot;] build-backend = &quot;poetry.masonry.api&quot; This changed two files, pyrpoject.toml (which we printed above) and poetry.lock (a record of all the packages and exact versions of them that poetry downloaded for this project). These changes are important for our package, so let’s commit them to version control as well: $ git add . $ git commit -m &quot;added pandas as a dependency&quot; [master f5c146d] added pandas as a dependency 2 files changed, 108 insertions(+) create mode 100644 poetry.lock For those of you who have used requirements.txt before with pip, you can think of poetry.lock as the poetry equivalent of that file. You can even generate a requirements.txt from poetry via poetry export -f requirements.txt &gt; requirements.txt if needed. 2.6 Package documentation 2.6.1 Reading and rendering the docs locally For the users of your code (include your future self!) we need to have readable and accessible documentation expressing how to install your package, and how to use the user-facing functions within it. The Python packaging ecosystem has a tool to help you make this process more efficient - Sphinx. In the Cookiecutter template we provided you to set-up this package, there is a basic docs template that the Cookiecutter progam filled in with the information you entered interactively when you ran cookiecutter https://github.com/UBC-MDS/cookiecutter-ubc-mds.git. These files live in the doc directory and are .rst (reStructuredText markup language) filetype. This is a lightweight markup language that works similar to Markdown but uses different syntax. The templates provided to you here are fairly well formatted already, so you do not have to change the .rst formatting, however if you are interested in doing so, you can see this CheatsSheet to get started. First, we need to install sphinx as a development dependency using poetry. $ poetry add --dev sphinx Note the use of --dev to specify a development, rather than a package function dependency. If you look in pyproject.toml you will see that sphinx gets added under the [tool.poetry.dev-dependencies] section as opposed to the [tool.poetry.dependencies] section (where package function dependencies get installed, such as pandas in this example). Next, to render the help documents locally from .rst to .html we navigate into the docs directory and then run the Makefile there, directing it to run the .html target: $ cd docs $ poetry run make html Note 1: we append poetry run infront of most of our commands in this Python package workflow so that we ensure we are using the software tools installed in the poetry virtual environment. Note 2: you may see many red warnings, these can be ignored and are meerely suggestions on how to improve your docs if you wish. If we now look inside our docs directory we see that it has expanded, and the rendered .html files live in _build/html. We can open _build/html/index.html to view our docs locally on our laptop, they should look something like this: If we click on the “Module Index” link under the heading “Indices and tables” we get a “Your file was not found message”: This is because we haven’t written any documentation for our package function. Let’s do that now by adding a numpy-style docstring to the catbind function in foocat/foocat.py as shown below: import pandas as pd def catbind(a, b): &quot;&quot;&quot; Concatenates two pandas categoricals. Parameters ---------- a : pandas.core.arrays.categorical.Categorical A pandas categorical. b : pandas.core.arrays.categorical.Categorical A pandas categorical that you wish to concatenate to a. Returns ------- pandas.core.arrays.categorical.Categorical The new concatenated pandas categorical. Examples -------- &gt;&gt;&gt; from foocat import foocat &gt;&gt;&gt; a = pd.Categorical([&quot;character&quot;, &quot;hits&quot;, &quot;your&quot;, &quot;eyeballs&quot;]) &gt;&gt;&gt; b = pd.Categorical([&quot;but&quot;, &quot;integer&quot;, &quot;where it&quot;, &quot;counts&quot;]) &gt;&gt;&gt; foocat.catbind(a, b) [character, hits, your, eyeballs, but, integer, where it, counts] Categories (8, object): [but, character, counts, eyeballs, hits, integer, where it, your] &quot;&quot;&quot; concatenated = pd.concat([pd.Series(a.astype(&quot;str&quot;)), pd.Series(b.astype(&quot;str&quot;))]) return pd.Categorical(concatenated) Now we can use a sphinx extension (napolean) in combination with autodoc to render our numpy-styled docstring into a modules page on our docs. To do this we need to install another dev dependency: $ poetry add --dev sphinxcontrib-napoleon Note - to use this, we also have to add extensions = ['sphinx.ext.napoleon'] in the conf.py file in the docs directory, but we have taken care of this for you with our Cookiecutter template. Then we can change back to our root foocat directory, use sphinx-apidoc and poetry to re-render our docs: $ cd .. $ poetry run sphinx-apidoc -f -o docs/source foocat $ cd docs $ poetry run make html Now when we click on the “Module Index” link under the heading “Indices and tables” we see a webpage that has a link to our module, foocat.foocat: And we can click on that to see the docs for foocat.foocat.catbind. Which should look roughly like this: Another hurray! 🎉🎉🎉 Let’s commit this to version control and push to our remote: $ cd .. $ git add . $ git commit -m &quot;generated and render docs for local viewing&quot; $ git push [master ae68b4e] generated and render docs for local viewing 5 files changed, 471 insertions(+), 8 deletions(-) rewrite foocat/foocat.py (77%) create mode 100644 docs/source/foocat.rst create mode 100644 docs/source/modules.rst 2.6.2 Reading and rendering the docs remotely To share these docs online, we need to link our GitHub repository to Read the Docs (where we will build and host our docs remotely). To do this: Visit https://readthedocs.org/ and click on “Sign up”; Select “Sign up with GitHub”; Click “Import a Project”; Click “Import Manually”; Fill in the project details by providing a package name (this must be a unique name, we’ve already taken “foocat” so perhaps try “foocat[your initials]”), the repository URL, and leave the rest as is. Click “Next”; Click “Build version”. After following the steps above, your docs should get successfully built on Read the Docs and you should be able to access them via the “View Docs” button on the build page, or from the link that Cookiecutter created for your on your repositories README.md file. Note: for Read the Docs to work with the poetry package workflow you need to have a .readthedocs.yml in the root of your Python package. We have created this for you using Cookiecutter and you can view it here. 2.7 Testing We have interactively taken catbind for a test drive, but to prove to our future self and others that our code does in fact do what it is supposed to do, let’s write some formal unit tests. In Python packages, our tests live inside the test directory, typically in a file called test_&lt;module_name&gt;.py, thus for this package this is tests/test_foocat.py. Let’s add a unit test (as a function named test_catbind) for our catbind function there now: from foocat import foocat import pandas as pd def test_catbind(): a = pd.Categorical([&quot;character&quot;, &quot;hits&quot;, &quot;your&quot;, &quot;eyeballs&quot;]) b = pd.Categorical([&quot;but&quot;, &quot;integer&quot;, &quot;where it&quot;, &quot;counts&quot;]) assert ((foocat.catbind(a, b)).codes == [1, 4, 7, 3, 0, 5, 6, 2]).all() assert ((foocat.catbind(a, b)).categories == [&quot;but&quot;, &quot;character&quot;, &quot;counts&quot;, &quot;eyeballs&quot;, &quot;hits&quot;, &quot;integer&quot;, &quot;where it&quot;, &quot;your&quot;]).all() Note - given that we use pd.Categorical to create objects to test on, we have to import the pandas package in our test file. We only have a single function to test for this package, and so we could run this file by opening up an interactive Python session, importing the file, and then calling the test_catbind function. But as our packages have more and more functions, and more and more modules (or submodules) then we want to automate this workflow. In the Python package ecosystem one way we can do this is to use pytest. A single call to pytest from the root of a project will look for all files in the tests directory, import all files prefixed with test* and then call all functions prefixed with test*. Pretty nice eh? To do this, we first add pytest as a dev dependency via poetry: $ poetry add --dev pytest Then to run the tests, we use poetry to run pytest in our virtual environment: $ poetry run pytest We get no error returned to us, indicating that our tests passed, Hurray! This suggests that the code we wrote is correct (at least to our test specifications)! Now we can share this with the world by putting these under local and remote version control: $ git add . $ git commit -m &quot;added unit tests for catbind&quot; $ git push Note: It is very possible that your tests are correct and your function passes the tests yet the build button on the GitHub repository README still says “build: failing”. This is because the build continuous integration has more checks than just whether the tests pass, for example, it also checks code style using a tool called flake8. This and the other build checks are the subject of later chapters in this book. 2.8 Building your package and publishing to testPyPI Python packages are generally shared via the PyPI package index. However, when we are just starting to develop packages, and/or at the development stage of our package, we typically first check that everything works by submitting to testPyPi. poetry has a command called publish which we can use to do this, however it’s default is to publish to PyPI. We can add this to the list of repositories poetry knows about via: $ poetry config repositories.test-pypi https://test.pypi.org/legacy/ Before we send our package, we first need to build it to source and wheel distributions (the format that PyPI distributes and something you’ll learn more about in the next chapter Package structure and state) using poetry build: $ poetry build And, finally, now to publish to testPyPI we can use poetry publish (you will be prompted for your testPyPI username and password, sign up for one if you have not already done so): $ poetry publish -r test-pypi Now you should be able to visit your package on testPyPI (e.g., https://test.pypi.org/project/foocat/) and download it from there using pip via: pip install -i https://test.pypi.org/simple/ foocat "],
["package-structure.html", "3 Package structure and state 3.1 Package states 3.2 Modules 3.3 Python packages 3.4 Source distribution packages 3.5 Binary distribution packages 3.6 Poetry and pyproject.toml 3.7 Installed packages 3.8 Imported Packages 3.9 Packaging Python Applications", " 3 Package structure and state The previous chapter demonstrated how to develop a Python package from scratch with the help of Python poetry. This chapter now takes a more in-depth look at packaging in Python. Often, developers don’t think about packaging until their code is actually written - but we’ll learn that thinking about packaging before even writing any code is very useful! This chapter is a somewhat Pythonified version of the Package Structure and State chapter of the R Packages book written by Jenny Bryan and also draws on information from the Python Packaging Authority. 3.1 Package states By “package” here we mean the code that you wish to bundle up and distribute. In Python, your package can be in several different states depending on its complexity, target audience, and stage of development. The ones we’ll talk about here are: modules packages source distributions binary distributions installed packages in-memory packages You’ve already seen some of the commands that put packages into these various states. For example, poetry build at the CL or import in a Python session. In the following sections, we’ll be giving those operations some context. The Python packaging workflow. 3.2 Modules A module is any Python .py file which may consist of Python functions, classes, variables, and/or runnable code. Modules can be useful for sharing simple Python code amongst compatible Python versions. A module that relies only on the standard Python library can easily be distributed and used by others (on the appropriate version of Python). In this way, a single module can be thought of as a package. For example, consider a module simple_math.py that contains the functions list_range and odd_even: def list_range(x): return max(x) - min(x) def odd_even(x): if x % 2: print(&#39;x is odd.&#39;) else: print(&#39;x is even.&#39;) If the module simple_math.py is in your working directory then you can import the module using: import simple_math # imports the entire module. Functions can then be accessed via dot notation, e.g., simple_math.list_range() from simple_math import list_range # import only list_range function from simple_math import odd_even # import only odd_even function from simple_math import * # import all functions Because modules are single files they can easily be shared to others by e.g., email, GitHub, Slack, etc. Another user would simply place the module in their working directory to use it. However, this method of distribution does not scale well in cases of multiple files, if your code depends on additional libraries, or needs a specific version of Python. 3.3 Python packages Projects consisting of multiple Python .py files (i.e., modules) are, by their nature, harder to distribute. If your project consists of multiple files, it is typical to organise it into a directory structure. Any directory containing Python files can comprise a Python “package”. While we’ve been using the term “package” generically so far, it does have a specific meaning in Python and it’s important to make clear the distinction between “modules” and “packages”. As described in the previous section, any Python .py file is a module. In contrast, a package is a directory containing module(s) and/or additional package(s) (sometimes called “nested packages” or “subpackages”) along with an __init__.py file. An __init__.py file is required to make Python treat a directory as a package (as opposed to it simply being a plain-old directory of Python files); in the simplest case __init__.py is an empty file, but it can also execute initialization code for the package upon import (read more here). Packages allow us to organise our Python code and intuitively access it using “dotted module names”. Consider having the following two packages in your working directory: A package containing modules: pkg1 ├── __init__.py ├── simple_math.py └── advanced_math.py A package containing nested packages: pkg2 ├── __init__.py ├── simple │ ├── __init__.py │ └── simple_math.py ├── advanced ├── __init__.py └── advanced_math.py Modules can be accessed using dot notation. For example: from pkg1 import simple_math # import simple_math module from pkg1 from pkg2.simple import simple_math # import simple_math module from pkg2 It would be possible to share a package by transferring all the files that comprise the package (keeping the directory structure intact) to another user, who could then use the package if it were placed in their working directory. However, just like single modules, this method of distribution does not scale well, makes it hard to support or update your code, and won’t work if your code depends on additional libraries, or needs a specific version of Python. We need a more efficient and reliable way to package and distribute our code which leads us to “source distribution packages” and “binary distribution packages” which are described below. 3.4 Source distribution packages A “distribution package” (often referred to simply as a “distribution”) is a single archive of the Python packages, modules and other files that make up your project. Having a single archive makes it easier to distribute your code to the wild. The base distribution format is called a “source distribution” (sdist). An sdist is a compressed archive (e.g., .tar.gz or .zip) of your package. Essentially, an sdist provides all of the metadata and source files needed for building and installing your code. You can read more about source distributions here. The standard tool in Python for creating sdists (and binary distributions, which we’ll explore in the next section) is setuptools. Note that, as we saw in The Whole Game chapter we prefer to use Poetry (as a simpler and more intuitive alternative to setuptools) to create distribution packages of our Python code. We’ll discuss Poetry in the later section Poetry and pyproject.toml. As a very simple example, consider the following directory which now contains a setup.py file. The setup.py file is a standard file that helps setuptools build your sdist and it is described in detail here. root ├── pkg1 │ ├── __init__.py │ ├── simple_math.py │ └── advanced_math.py └── setup.py Your sdist can then be built by changing to the root directory and running the following command: $ python setup.py sdist This will create an archive file (.tar.gz by default) of your project which is your sdist. If your code is pure Python then an sdist is a perfectly acceptable way to distribute your code, and a user could install it using: $ python setup.py install You could also share your sdist to PyPI from which a user could install it using pip install. It’s important to note that installing a package actually adds the package to your default installation directory (more on that in section Installed packages) such that it is accessible outside of your working directory - this is a key difference to simply sharing code as a module or package as we explored in the last two sections. We recommend consulting the The Hitchhiker’s Guide to Packaging and the Python docs for more information on creating and distributing source distributions. Some notable examples of Python sdists include: Django, hyperlink, requests. However, if your code relies on any non-Python code/libraries/packages, a binary distribution, described in the next section, is a much better way to package and distribute your code. In fact, binary distributions are always preferred by Python’s installer pip and so are typically recommended even for pure-Python packages. 3.5 Binary distribution packages Binary distributions are a type of distribution that contains compiled extensions. One of the most powerful features of Python is its ability to interoperate with libraries written in other languages, for example, C, Fortran, etc. Not all end-users will have the tools, experience, or time to build packages containing extensions written in these other languages, so binary distributions are how you make life as easy as possible for installers of your code. Like a source distribution, a binary distribution is a single artefact. However, binary distributions are “pre-built” meaning that, unlike source distributions, they do not require a build step before installation. The main binary distribution format used by Python is called a wheel (more on that later). Python’s package installer, pip, prefers wheels because installation is always faster than building and installing from an sdist (even if your sdist is pure Python it still involves a build step to build out the metadata from the setup.py file). As an example, much of the commonly used Python library NumPy is implemented as C extensions. The existence of wheels means that a user can, for example, simply run pip install numpy to install NumPy from PyPi, as opposed to having to build it from source with the help of a C compiler, amongst other requirements. If you’re feeling particularly masochistic you can try to build NumPy from source following these instructions from the NumPy docs. Binary distributions are platform specific (i.e., Windows, Mac, Linux). As a result, binary distributions are usually provided with their corresponding source distributions; if you don’t upload wheels of your code for every operating system, end-users will still be able to build it from source. Take a look at the downloadable file list of NumPy on PyPi - you’ll see wheels for most common platforms, as well as the source distribution at the bottom of the list. Wheels actually come in three flavours (which you can read more about here): Universal wheels: pure Python and support Python 2 and 3. Can be installed anywhere using pip. Pure Python wheels: pure Python but don’t support both Python 2 and 3 Platform wheels: binary packages specific to certain platforms as a result of containing compiled extensions. You can actually tell a lot about a wheel from the name itself which follows a strict naming convention: {distribution}-{version}(-{build tag})?-{python tag}-{abi tag}-{platform tag}.whl.. For example, the NumPy wheel numpy-1.18.1-cp37-cp37m-macosx_10_9_x86_64.whl tells us that: The distribution is NumPy v1.18.1; It is made for Python 3.7; It is specific to the macosx_10_9_x86_64 platform (i.e, this is a “platform wheel” because it is platform-specific). Building wheels is similar to building source distributions with setuptools. Once your setup.py and related files are ready. We recommend taking a look at the Python Packaging User Guide tutorial if you’re interested in using setuptools to properly build the sdist and wheel of your project. 3.6 Poetry and pyproject.toml The previous sections gave a high level overview of Python’s standard packaging options and tools. However, in The Whole Game Chapter we used Poetry to create a toy Python package - so where does this fit into the Python packaging landscape? Well, in the previous sections on Source distribution packages and Binary distribution packages, we really only touched the tip of the iceberg of Python packaging. When creating a package there’s a lot of customisation to think about with your setup.py file, and a host of other files we didn’t even talk about (e.g., requirements.txt, setup.cfg)! Needless to say, packaging in Python can be hard to understand, especially for beginners. These words echo the sentiments of Poetry’s creator Sébastien Eustace and the motivation for creating the tool: “Packaging systems and dependency management in Python are rather convoluted and hard to understand for newcomers. Even for seasoned developers it might be cumbersome at times to create all files needed in a Python project: setup.py, requirements.txt, setup.cfg, MANIFEST.in, and the newly added Pipfile. So I wanted a tool that would limit everything to a single configuration file to do: dependency management, packaging and publishing.” That “single configuration file” is pyproject.toml (you can read more about .toml files here). Essentially, Poetry is based on all the concepts of sdists and wheels discussed previously - it just simplifies and streamlines the whole packaging process in an intuitive way. In fact, the poetry build command you’ve seen previously, actually creates the sdist and wheel distributions of your package for you (depending on how you’ve configured your project). Python packaging gamut. Modified after The Packaging Gradient by Mahmoud Hashemi. 3.7 Installed packages An installed package is a distribution that’s been decompressed, built (in the case of an sdist) and then copied to your chosen installation directory. The default “chosen installation directory” varies by platform and by how you built/installed Python. For example, I installed Python using the miniconda distribution and my default directory for package installation is /Users/tbeuzen/miniconda3/lib/python3.7/site-packages. “Installing” a distribution (e.g., by pip install XXX) is really a two-step process: 1) building the package, and 2) installing the package. Using wheels takes out the first step, meaning we only need to install. The install step is simple, all it has to do is copy all the decompressed files to the appropriate directory. In fact, we can manually install a package if we want to by manually decompressing a wheel and copying the files to their appropriate locations - there’s no real reason to do this because it’s far more effort than using a single one-liner at the CL, it does not resolve dependencies so could break your installation, and probably has other unwanted side-effects. However, it’s a nice way to learn about the package installation process, so if you’d like to give it a go, you can try the following steps (which assume you’re using MacOS and the conda package manager): Create a new virtual environment to act as a safe, test playground. As a conda user, the CL command for me to create a new empty virtual environment called “manualpkg” and including Python 3.7 is conda create -name manpkg python=3.7. Be sure to activate the environment once it has been created (i.e., conda activate manualpkg); You can find a toy wheel to download in the GitHub repository of this book here (although you can try this manual installation procedure with a wheel downloaded from any source, e.g., PyPi). Download the wheel into the site-packages directory of the manualpkg environment, which for me was located at /Users/tbeuzen/miniconda3/envs/manualpkg/lib/python3.7/site-packages/; From the CL, cd to the site-packages directory of the manualpkg environment; At this point, there is no toy_pkg package installed for our virtual environment to access. So starting a Python session from the CL (python) and trying import toy_pkg will fail because even though the wheel is there, we haven’t decompressed it yet; From the CL we need to run unzip toy_pkg-0.0.1-py3-none-any.whl; You’ll now find two fresh directories: toy_pkg and toy_pkg-0.0.1.dist-info; From the CL start a Python session by typing python and try the following: &gt;&gt;&gt; from toy_pkg.toy_module import test_function &gt;&gt;&gt; test_function() You manually installed the toy_pkg example! Well done! 3.8 Imported Packages We now arrive at our last package state, the “imported package”. This state is associated with a command that is familiar to everyone that uses Python: import somemodule You can read about the import system in detail in the Python documentation. Briefly, the import statement comprises two operations: it searches for the named module; and, then binds the results of that search to a name in the local namespace. Note that for efficiency, each module is only imported once per interpreter session. If you modify your module, you can’t just re-run your import statement (as that name in the namespace is already populated and won’t be re-loaded). Instead, you have to restart your interpreter or force the import using importlib.reload(), but this is inefficient when working with multiple modules. 3.9 Packaging Python Applications In this chapter we’ve only talked about packaging and distributing reusable Python code, a process which is really aimed at developers and audiences familiar with Python. While it’s outside the scope of this book, it’s also possible to package and distribute entire Python applications, that is, software that is not meant to be developed on but rather to use. Some good exmaples of Python-based applications are Sublime Text, EVE online, and Reddit. There are a lot of options available for packaging and distributing Python applications and we recommend watching the excellent talk by Mahmoud Hashemi “The Packaging Gradient” to learn more. To give you an idea of the available options, the figure below shows a summary of the different options discussed by Mahmoud for packaging Python applications. Python application packaging gamut. Modified after The Packaging Gradient by Mahmoud Hashemi. "]
]
